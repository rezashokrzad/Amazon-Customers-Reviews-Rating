{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TxMM_Shokrzad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcw5hbbcsgPj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9tDr9KGsiiz",
        "outputId": "6806c544-5710-48c6-e976-757fab13eac1"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "## for data\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "## for plotting\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "## for processing\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "## for bag-of-words\r\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\r\n",
        "## for explainer\r\n",
        "# from lime import lime_text\r\n",
        "## for word embedding\r\n",
        "import gensim\r\n",
        "import gensim.downloader as gensim_api\r\n",
        "## for deep learning\r\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\r\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhpS5CeCvJRL"
      },
      "source": [
        "## Connect to GoogleDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsLlx8-QvL2o",
        "outputId": "66c22638-3fd0-4261-b80e-bb98cec39837"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n",
        "!cd /gdrive/MyDrive/Dataset/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiW761qKvNP2"
      },
      "source": [
        "#name the columns of reviews\r\n",
        "df = pd.read_csv(\"/gdrive/My Drive/Dataset/TxMM_dataset_Process.csv\" , sep=',')\r\n",
        "x_title = df.values[:,1]  #review.title\r\n",
        "x_full = df.values[:,2] #review.text\r\n",
        "label = df.values[: , 0]  #review.rating\r\n",
        "#creating two differnet classes\r\n",
        "label[label != 5] = 0\r\n",
        "label[label == 5] = 1\r\n",
        "#allocating missing ratings to class1 \r\n",
        "NN = []\r\n",
        "for i in range(len(label)):\r\n",
        "  try:\r\n",
        "    (int)(label[i])\r\n",
        "    continue\r\n",
        "  except:\r\n",
        "    NN.append(i)\r\n",
        "\r\n",
        "label[NN] = 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiLkpXu8weAS"
      },
      "source": [
        "## Load word2vec-google-news-300 pretrained model\r\n",
        "this may take a few minutes because the size of model is about 1.6 GB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06s8H25Nv0Fh"
      },
      "source": [
        "nlp = gensim_api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8LCqvt-eQ4N"
      },
      "source": [
        "## PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0LVtFXRwjku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc60280a-0795-4eb5-d76b-e48374239591"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "#stopwords.words('english')\r\n",
        "\r\n",
        "stemmer=nltk.PorterStemmer()\r\n",
        "\r\n",
        "\r\n",
        "corpus_full = []\r\n",
        "i = 0\r\n",
        "for str_full in x_full:\r\n",
        "   if pd.isna(x_title[i]):\r\n",
        "     x_title[i] = ''\r\n",
        "   if pd.isna(str_full):\r\n",
        "     str_full = ''\r\n",
        "   string = str(x_title[i]) + ' ' + str(x_title[i]) + ' ' + str_full   # emphasizing on title\r\n",
        "   \r\n",
        "   i = i + 1\r\n",
        "   if i % 500 == 1:\r\n",
        "    print(str(i))   # to check the progress\r\n",
        "   string = [stemmer.stem(word.lower()) for word in string if not word.lower() in set(stopwords.words('english'))]\r\n",
        "   string = ' '.join(string)\r\n",
        "   lst_words = string.lower().split()\r\n",
        "   lst_grams = [\" \".join(lst_words[i:i+1]) \r\n",
        "               for i in range(0, len(lst_words), 1)]\r\n",
        "   corpus_full.append(lst_grams)\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "1\n",
            "501\n",
            "1001\n",
            "1501\n",
            "2001\n",
            "2501\n",
            "3001\n",
            "3501\n",
            "4001\n",
            "4501\n",
            "5001\n",
            "5501\n",
            "6001\n",
            "6501\n",
            "7001\n",
            "7501\n",
            "8001\n",
            "8501\n",
            "9001\n",
            "9501\n",
            "10001\n",
            "10501\n",
            "11001\n",
            "11501\n",
            "12001\n",
            "12501\n",
            "13001\n",
            "13501\n",
            "14001\n",
            "14501\n",
            "15001\n",
            "15501\n",
            "16001\n",
            "16501\n",
            "17001\n",
            "17501\n",
            "18001\n",
            "18501\n",
            "19001\n",
            "19501\n",
            "20001\n",
            "20501\n",
            "21001\n",
            "21501\n",
            "22001\n",
            "22501\n",
            "23001\n",
            "23501\n",
            "24001\n",
            "24501\n",
            "25001\n",
            "25501\n",
            "26001\n",
            "26501\n",
            "27001\n",
            "27501\n",
            "28001\n",
            "28501\n",
            "29001\n",
            "29501\n",
            "30001\n",
            "30501\n",
            "31001\n",
            "31501\n",
            "32001\n",
            "32501\n",
            "33001\n",
            "33501\n",
            "34001\n",
            "34501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "00V3jPSrv1lT",
        "outputId": "fbe49aaa-c6f9-490b-8b9f-890dda2c2f5d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews.rating</th>\n",
              "      <th>reviews.title</th>\n",
              "      <th>reviews.text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Kindle</td>\n",
              "      <td>This product so far has not disappointed. My c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>very fast</td>\n",
              "      <td>great for beginner or experienced person. Boug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Beginner tablet for our 9 year old son.</td>\n",
              "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Good!!!</td>\n",
              "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Fantastic Tablet for kids</td>\n",
              "      <td>I bought this for my grand daughter when she c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reviews.rating  ...                                       reviews.text\n",
              "0             5.0  ...  This product so far has not disappointed. My c...\n",
              "1             5.0  ...  great for beginner or experienced person. Boug...\n",
              "2             5.0  ...  Inexpensive tablet for him to use and learn on...\n",
              "3             4.0  ...  I've had my Fire HD 8 two weeks now and I love...\n",
              "4             5.0  ...  I bought this for my grand daughter when she c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJhcrSpOYCQa"
      },
      "source": [
        "## tokenize text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubbYfh0xYDa6"
      },
      "source": [
        "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \r\n",
        "                     oov_token=\"NaN\", \r\n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "tokenizer.fit_on_texts(corpus_full)\r\n",
        "dic_vocabulary = tokenizer.word_index"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmYbC1zkYMRt"
      },
      "source": [
        "## create sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tigsxkY0Xzcj"
      },
      "source": [
        "full_text2seq = tokenizer.texts_to_sequences(corpus_full)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQhtp1Q2YU_J"
      },
      "source": [
        "## padding sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyeIZ-LYYVrW"
      },
      "source": [
        "Dim = 30\r\n",
        "\r\n",
        "X_full = kprocessing.sequence.pad_sequences(full_text2seq, \r\n",
        "                    maxlen=Dim, padding=\"post\", truncating=\"post\")"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2FQKzagYgNZ"
      },
      "source": [
        "## start the matrix (length of vocabulary x vector size) with all 0s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbv1LwmYnfj"
      },
      "source": [
        "embeddings = np.zeros((len(dic_vocabulary)+1, 30))\r\n",
        "for word,idx in dic_vocabulary.items():\r\n",
        "    ## update the row with vector\r\n",
        "    try:\r\n",
        "        embeddings[idx] =  nlp[word]\r\n",
        "    ## if word not in model then skip and the row stays all 0s\r\n",
        "    except:\r\n",
        "        pass"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1S4nWI0ZBur"
      },
      "source": [
        "N = np.size(X_full,0)\r\n",
        "X_Features = np.zeros((N , 30 * Dim))\r\n",
        "for i in range(N):\r\n",
        "  for j in range(Dim):\r\n",
        "    e = embeddings[X_full[i,j]]\r\n",
        "    X_Features[i , j * 30 : (j+1) * 30] = e"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFR6aC6tLeRS"
      },
      "source": [
        "# Build TF graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCevgnE2OcOc"
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sCNn1NhOTIg"
      },
      "source": [
        "## Create placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2555umSMLo_"
      },
      "source": [
        "X = tf.placeholder(shape=[None, 30, Dim,1], dtype=tf.float32, name=\"X\")\n",
        "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Br_L2VtOs6o"
      },
      "source": [
        "## Convolutional layer properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECgR7j5tPDxC"
      },
      "source": [
        "conv1_params = {\n",
        "    \"filters\": 32,\n",
        "    \"kernel_size\": [5 , 1],\n",
        "    \"strides\": 1,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu,\n",
        "}\n",
        "conv2_params = {\n",
        "    \"filters\": 4,\n",
        "    \"kernel_size\": [5 , 1],\n",
        "    \"strides\": 1,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu,\n",
        "}\n",
        "\n",
        "pool_params = {\n",
        "    \"ksize\" : [1,3,1,1],\n",
        "    \"strides\" : [1,3,1,1],\n",
        "    \"padding\" : \"VALID\"\n",
        "}\n",
        "\n",
        "pool_params2 = {\n",
        "    \"ksize\" : [1,4,1,1],\n",
        "    \"strides\" : [1,4,1,1],\n",
        "    \"padding\" : \"VALID\"\n",
        "}"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrehGAIcRBnE"
      },
      "source": [
        "## first conv layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L5C3-3-QZbv"
      },
      "source": [
        "conv1 = tf.layers.conv2d(X, **conv1_params)  # shape(?, 26, 30, 32)\n",
        "conv2 = tf.layers.conv2d(X, **conv1_params)\n",
        "conv3 = tf.layers.conv2d(X, **conv1_params)\n",
        "conv4 = tf.layers.conv2d(X, **conv1_params)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhZ1ZAdFSH8z"
      },
      "source": [
        "## first max pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33OlHABKUjzO"
      },
      "source": [
        "pool1 = tf.layers.batch_normalization(tf.nn.max_pool(conv1, **pool_params)) # shape(?, 8, 30 ,32)\n",
        "pool2 = tf.layers.batch_normalization(tf.nn.max_pool(conv2, **pool_params))\n",
        "pool3 = tf.layers.batch_normalization(tf.nn.max_pool(conv3, **pool_params))\n",
        "pool4 = tf.layers.batch_normalization(tf.nn.max_pool(conv4, **pool_params))"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHZPKJr2VMU6"
      },
      "source": [
        "concatLayer = tf.concat([pool1 , pool2 , pool3 , pool4 ] , axis = 1) #shape(?,32,30,32)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63wq_KJ0SNFv"
      },
      "source": [
        "## Second conv  layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZ9DXj6XawW"
      },
      "source": [
        "secondConv = tf.layers.conv2d(concatLayer, **conv2_params) #shape(?, 28, 30, 4)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQWd6k6lX3cN"
      },
      "source": [
        "## second max pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1zn3AyBX5aG"
      },
      "source": [
        "secondPool = tf.nn.max_pool(secondConv, **pool_params) #shape(?, 9, 30, 4)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rg8gFqiYEvS"
      },
      "source": [
        "## Third conv layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKy9MQsQXmSd"
      },
      "source": [
        "thirdConv = tf.layers.conv2d(secondPool,**conv2_params) #shape(?, 5, 30, 4)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgOKQhGXYmSG"
      },
      "source": [
        "## third max pool layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN6Zn5f_YtZY"
      },
      "source": [
        "thirdPool = tf.layers.batch_normalization(tf.nn.max_pool(thirdConv , **pool_params2)) # additive batch_normalization #shape(?, 1, 30, 4)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR6YzEZvYhTY"
      },
      "source": [
        "flattenLayer = tf.layers.flatten(thirdPool) #shape(?, 120)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGXnFlnOaj-U"
      },
      "source": [
        "## Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPUjvMaOanIo"
      },
      "source": [
        "def init_weights(shape):\n",
        "  init_random_dist = tf.truncated_normal(shape , stddev = 0.1)\n",
        "  return tf.Variable(init_random_dist)\n",
        "\n",
        "def init_bias(shape):\n",
        "  init_bias_vals = tf.constant(0.1 , shape = shape)\n",
        "  return tf.Variable(init_bias_vals)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiN8oU6UapAd"
      },
      "source": [
        "input_size = (int)(flattenLayer.get_shape()[1])\n",
        "W = init_weights([input_size , 32])\n",
        "b = init_bias([32])\n",
        "dense1 = tf.matmul(flattenLayer , W) + b\n",
        "relu1 = tf.nn.relu(dense1)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48vU-EdDirJn"
      },
      "source": [
        "num_class = 2\n",
        "Wf = init_weights([32 , num_class])\n",
        "bf = init_bias([num_class])\n",
        "y_predicted = tf.matmul(relu1 , Wf) + bf"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnWaFDKkrBxC"
      },
      "source": [
        "## Lost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHhNylogjbaQ"
      },
      "source": [
        "T = tf.one_hot(y, depth = num_class)\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = T , logits = y_predicted,))\n",
        "loss = cross_entropy + 0.001 * (tf.norm(W) + tf.norm(Wf))"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOZgV3kjrA1D"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer (learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
        "train = optimizer.minimize(loss)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z71wRO4lF2Jl"
      },
      "source": [
        "y_pred_argmax = tf.argmax(y_predicted, axis=1)\n",
        "correct = tf.equal(y, y_pred_argmax)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8K-9a2sB2gA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5unymN1svvK1"
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OReyl1AJf2ax"
      },
      "source": [
        "def getNextBatch(batch_size):\r\n",
        "  Ntr = np.size(Xtr,0)\r\n",
        "  idx = np.random.randint(0 , Ntr , batch_size)\r\n",
        "  Xbatch = Xtr[idx,:]\r\n",
        "  Ybatch = y_train[idx]\r\n",
        "  return Xbatch , Ybatch"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUOYCMaxgt5J"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "testPercent = 0.30\r\n",
        "Xtr, Xts, y_train, y_test = train_test_split(X_Features,label, test_size=testPercent, random_state = 11)\r\n",
        "Ntrain = np.size(Xtr,0)\r\n",
        "Ntest = np.size(Xts,0)"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39OsxIv4B_hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198fde07-72fb-432b-b5bf-8cfdaa4527e3"
      },
      "source": [
        "#CNN\n",
        "n_epochs = 10\n",
        "batch_size = 10\n",
        "n_iterations_per_epoch = Ntrain // batch_size\n",
        "n_iterations_test = Ntest\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(n_epochs):\n",
        "    for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "        X_batch, y_batch = getNextBatch(batch_size)\n",
        "        # Run the training operation and measure the loss:\n",
        "        _, loss_train = sess.run(\n",
        "            [train, loss],\n",
        "            feed_dict = {X: X_batch.reshape([-1, 30, Dim, 1]),\n",
        "                         y: y_batch})\n",
        "        print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
        "                  iteration, n_iterations_per_epoch,\n",
        "                  iteration * 100 / n_iterations_per_epoch,\n",
        "                  loss_train),\n",
        "              end=\"\")\n",
        "\n",
        "    # At the end of each epoch,\n",
        "    # measure the validation loss and accuracy:\n",
        "    loss_tests = []\n",
        "    acc_tests = []\n",
        "    step = 100\n",
        "    for iteration in range(0, n_iterations_test,step):\n",
        "        X_batch = Xts[iteration:iteration+step,:]\n",
        "        y_batch = y_test[iteration:iteration+step]\n",
        "        \n",
        "        loss_test, acc_test = sess.run(\n",
        "                [loss, accuracy],\n",
        "                feed_dict = {X: X_batch.reshape([-1, 30, Dim, 1]),\n",
        "                             y: np.reshape(y_batch,[-1])})\n",
        "        loss_tests.append(loss_test)\n",
        "        acc_tests.append(acc_test)\n",
        "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
        "                  iteration, n_iterations_test,\n",
        "                  iteration * 100 / n_iterations_test),\n",
        "              end=\" \" * 10)\n",
        "    \n",
        "    loss_test = np.mean(loss_tests)\n",
        "    acc_test = np.mean(acc_tests)\n",
        "    print(\"\\rEpoch: {}  Test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
        "        epoch + 1, acc_test * 100, loss_test))\n",
        "    \n",
        "  print('Predicting final labels')\n",
        "  y_pred = []\n",
        "  for iteration in range(0, n_iterations_test):\n",
        "        X_batch = Xts[iteration,:]\n",
        "        y_batch = y_test[iteration]\n",
        "        \n",
        "        yp = sess.run(\n",
        "                [y_pred_argmax],\n",
        "                feed_dict = {X: X_batch.reshape([-1, 30, Dim, 1])})\n",
        "        y_pred.append(yp[:])"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Test accuracy: 69.0039%  Loss: 0.621052\n",
            "Epoch: 2  Test accuracy: 69.0039%  Loss: 0.620496\n",
            "Epoch: 3  Test accuracy: 69.0039%  Loss: 0.619761\n",
            "Epoch: 4  Test accuracy: 69.0039%  Loss: 0.619987\n",
            "Epoch: 5  Test accuracy: 69.0039%  Loss: 0.619709\n",
            "Epoch: 6  Test accuracy: 69.0039%  Loss: 0.619552\n",
            "Epoch: 7  Test accuracy: 69.0039%  Loss: 0.619658\n",
            "Epoch: 8  Test accuracy: 69.0039%  Loss: 0.620347\n",
            "Epoch: 9  Test accuracy: 69.0039%  Loss: 0.619947\n",
            "Epoch: 10  Test accuracy: 69.0039%  Loss: 0.619396\n",
            "Predicting final labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnn76jGC1AK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3279fc5e-c99d-400b-bb39-f8b25f637804"
      },
      "source": [
        "#SVM\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn import svm\r\n",
        "clf = svm.SVC()\r\n",
        "clf.fit(Xtr, y_train.astype(int))\r\n",
        "Yp2 = clf.predict(Xts)\r\n",
        "print(classification_report(y_test.astype(int),Yp2))"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      3223\n",
            "           1       0.69      1.00      0.82      7175\n",
            "\n",
            "    accuracy                           0.69     10398\n",
            "   macro avg       0.35      0.50      0.41     10398\n",
            "weighted avg       0.48      0.69      0.56     10398\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TIjcXTxI9ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4259de4-8d31-429e-99a8-60bd7e552464"
      },
      "source": [
        "#KNN\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\r\n",
        "neigh.fit(Xtr, y_train.astype(int))\r\n",
        "Yp5 = neigh.predict(Xts)\r\n",
        "print(classification_report(y_test.astype(int),Yp5))"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      3223\n",
            "           1       0.69      1.00      0.82      7175\n",
            "\n",
            "    accuracy                           0.69     10398\n",
            "   macro avg       0.35      0.50      0.41     10398\n",
            "weighted avg       0.48      0.69      0.56     10398\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPrSYBuN4xb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a91e55-df95-4d49-d2a2-412b9d685f86"
      },
      "source": [
        "# KNN + TF-IDF\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\r\n",
        "neigh.fit(Xtr, y_train.astype(int))\r\n",
        "Yp3 = neigh.predict(Xts)\r\n",
        "print(classification_report(y_test.astype(int),Yp3))"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.36      0.40      3286\n",
            "           1       0.73      0.80      0.76      7112\n",
            "\n",
            "    accuracy                           0.66     10398\n",
            "   macro avg       0.59      0.58      0.58     10398\n",
            "weighted avg       0.64      0.66      0.65     10398\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vpwYIMx5pxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c3701b-2daf-46f9-eb69-388382a1b54f"
      },
      "source": [
        "# DT + TF-IDF\r\n",
        "from sklearn import tree\r\n",
        "clf = tree.DecisionTreeClassifier()\r\n",
        "clf.fit(Xtr, y_train.astype(int))\r\n",
        "Yp4 = clf.predict(Xts)\r\n",
        "print(classification_report(y_test.astype(int),Yp4))\r\n"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.43      0.43      3286\n",
            "           1       0.74      0.74      0.74      7112\n",
            "\n",
            "    accuracy                           0.64     10398\n",
            "   macro avg       0.58      0.58      0.58     10398\n",
            "weighted avg       0.64      0.64      0.64     10398\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}